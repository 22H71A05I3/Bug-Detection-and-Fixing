{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shreyanshy53/Bug-Detection-and-Fixing/blob/main/BugDetectionProject/BugDetectionFixing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iTupoYXJYEk",
        "outputId": "4fe58348-88a5-4e22-cbb9-c427f8e5ad81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgzQzkPJKVT8",
        "outputId": "73f75f8d-b644-4972-b87a-f1c4608389af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files in BugDetectionProject: ['codenetpy_test.json', 'codenetpy_train.json', 'BugDetectionFixing.ipynb']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "data_path = \"/content/drive/MyDrive/BugDetectionProject\"\n",
        "print(\"Files in BugDetectionProject:\", os.listdir(data_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jd-6TwARKa6Y",
        "outputId": "f8412d3c-a177-41ae-ebe5-e5f956dd26d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total training samples: 43673\n",
            "Total testing samples: 10919\n",
            "Example sample: {'original_src': 'l=[]\\nwhile True:\\n    try:\\n        l.append(input())\\n    except SyntaxError:\\n        break\\nl.sort(reverse = True)\\nfor i in range(3):\\n    print l[i]', 'changed_src': 'l=[]\\nwhile True:\\n    try:\\n        l.append(input())\\n    except EOFError:\\n        break\\nl.sort(reverse = True)\\nfor i in range(3):\\n    print l[i]', 'problem_id': 'p00001', 'original_id': 's196059089', 'changed_id': 's508355022', 'language': 'Python', 'filename_ext': 'py', 'original_status': 'Runtime Error', 'returncode': 1, 'error_class': 'SyntaxError', 'error_class_extra': \"SyntaxError: Missing parentheses in call to 'print'. Did you mean print(l[i])?\", 'error': '  File \"/home/alex/Documents/research/bug-detection/../input/Project_CodeNet/data/p00001/Python/s196059089.py\", line 9\\n    print l[i]\\n          ^\\nSyntaxError: Missing parentheses in call to \\'print\\'. Did you mean print(l[i])?\\n', 'output': ''}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "train_file = \"/content/drive/MyDrive/BugDetectionProject/codenetpy_train.json\" #change the path\n",
        "test_file = \"/content/drive/MyDrive/BugDetectionProject/codenetpy_test.json\" #please change the path of dataset\n",
        "\n",
        "def load_json(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    if isinstance(data, dict) and \"data\" in data:\n",
        "        return data[\"data\"]\n",
        "    return data\n",
        "train_data = load_json(train_file)\n",
        "test_data = load_json(test_file)\n",
        "print(\"Total training samples:\", len(train_data))\n",
        "print(\"Total testing samples:\", len(test_data))\n",
        "print(\"Example sample:\", train_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLRnfXFoKgnB"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import tokenize\n",
        "from io import BytesIO\n",
        "import ast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3AHtfz-KjdB"
      },
      "outputs": [],
      "source": [
        "def python_tokenizer(code):\n",
        "    tokens = []\n",
        "    try:\n",
        "        token_stream = tokenize.tokenize(BytesIO(code.encode('utf-8')).readline)\n",
        "        for token in token_stream:\n",
        "            if token.type != tokenize.ENCODING:\n",
        "                tokens.append(token.string)\n",
        "    except Exception as e:\n",
        "        print(\"Tokenization Error:\", e)\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDmPjxQuKnTv"
      },
      "outputs": [],
      "source": [
        "def extract_ast_tokens(code):\n",
        "    try:\n",
        "        tree = ast.parse(code)\n",
        "        tokens = [node.__class__.__name__ for node in ast.walk(tree)]\n",
        "        return tokens\n",
        "    except Exception as e:\n",
        "        print(\"AST Parsing Error:\", e)\n",
        "        return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwRpOW7sKp7s",
        "outputId": "0028abc0-9815-4db4-ed37-654a207d01dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Data Keys: dict_keys(['data'])\n",
            "Test Data Keys: dict_keys(['data'])\n",
            "First Sample Keys: dict_keys(['original_src', 'changed_src', 'problem_id', 'original_id', 'changed_id', 'language', 'filename_ext', 'original_status', 'returncode', 'error_class', 'error_class_extra', 'error', 'output'])\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "train_path = \"/content/drive/MyDrive/BugDetectionProject/codenetpy_train.json\"\n",
        "test_path = \"/content/drive/MyDrive/BugDetectionProject/codenetpy_test.json\"\n",
        "with open(train_path, \"r\") as f:\n",
        "    train_data = json.load(f)\n",
        "with open(test_path, \"r\") as f:\n",
        "    test_data = json.load(f)\n",
        "print(\"Train Data Keys:\", train_data.keys())\n",
        "print(\"Test Data Keys:\", test_data.keys())\n",
        "\n",
        "first_sample = list(train_data[\"data\"])[0]\n",
        "print(\"First Sample Keys:\", first_sample.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQcI3iwmKtIe",
        "outputId": "702553d0-de32-4965-cc5a-45da2d421910"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Train Data Sample:\n",
            "{'original_src': 'l=[]\\nwhile True:\\n    try:\\n        l.append(input())\\n    except SyntaxError:\\n        break\\nl.sort(reverse = True)\\nfor i in range(3):\\n    print l[i]', 'changed_src': 'l=[]\\nwhile True:\\n    try:\\n        l.append(input())\\n    except EOFError:\\n        break\\nl.sort(reverse = True)\\nfor i in range(3):\\n    print l[i]', 'problem_id': 'p00001', 'original_id': 's196059089', 'changed_id': 's508355022', 'language': 'Python', 'filename_ext': 'py', 'original_status': 'Runtime Error', 'returncode': 1, 'error_class': 'SyntaxError', 'error_class_extra': \"SyntaxError: Missing parentheses in call to 'print'. Did you mean print(l[i])?\", 'error': '  File \"/home/alex/Documents/research/bug-detection/../input/Project_CodeNet/data/p00001/Python/s196059089.py\", line 9\\n    print l[i]\\n          ^\\nSyntaxError: Missing parentheses in call to \\'print\\'. Did you mean print(l[i])?\\n', 'output': ''}\n"
          ]
        }
      ],
      "source": [
        "print(\" Train Data Sample:\")\n",
        "print(train_data[\"data\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJgVIFv8Kvd3",
        "outputId": "1921ef82-bf4e-4df0-a9dd-66fa435c3e08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"original_src\": \"l=[]\\nwhile True:\\n    try:\\n        l.append(input())\\n    except SyntaxError:\\n        break\\nl.sort(reverse = True)\\nfor i in range(3):\\n    print l[i]\",\n",
            "    \"changed_src\": \"l=[]\\nwhile True:\\n    try:\\n        l.append(input())\\n    except EOFError:\\n        break\\nl.sort(reverse = True)\\nfor i in range(3):\\n    print l[i]\",\n",
            "    \"problem_id\": \"p00001\",\n",
            "    \"original_id\": \"s196059089\",\n",
            "    \"changed_id\": \"s508355022\",\n",
            "    \"language\": \"Python\",\n",
            "    \"filename_ext\": \"py\",\n",
            "    \"original_status\": \"Runtime Error\",\n",
            "    \"returncode\": 1,\n",
            "    \"error_class\": \"SyntaxError\",\n",
            "    \"error_class_extra\": \"SyntaxError: Missing parentheses in call to 'print'. Did you mean print(l[i])?\",\n",
            "    \"error\": \"  File \\\"/home/alex/Documents/research/bug-detection/../input/Project_CodeNet/data/p00001/Python/s196059089.py\\\", line 9\\n    print l[i]\\n          ^\\nSyntaxError: Missing parentheses in call to 'print'. Did you mean print(l[i])?\\n\",\n",
            "    \"output\": \"\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "print(json.dumps(train_data[\"data\"][0], indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUQWEUwZKySI",
        "outputId": "06efa03f-840d-4ab4-d951-08f976b74734"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Data Type: <class 'dict'>\n",
            "Test Data Type: <class 'dict'>\n",
            " 'data' key found in train_data! Length: 43673\n",
            "First Sample Keys: dict_keys(['original_src', 'changed_src', 'problem_id', 'original_id', 'changed_id', 'language', 'filename_ext', 'original_status', 'returncode', 'error_class', 'error_class_extra', 'error', 'output'])\n",
            " 'data' key found in test_data! Length: 10919\n",
            "First Sample Keys: dict_keys(['original_src', 'changed_src', 'problem_id', 'original_id', 'changed_id', 'language', 'filename_ext', 'original_status', 'returncode', 'error_class', 'error_class_extra', 'error', 'output'])\n"
          ]
        }
      ],
      "source": [
        "print(\"Train Data Type:\", type(train_data))\n",
        "print(\"Test Data Type:\", type(test_data))\n",
        "if \"data\" in train_data and isinstance(train_data[\"data\"], list):\n",
        "    print(\" 'data' key found in train_data! Length:\", len(train_data[\"data\"]))\n",
        "    print(\"First Sample Keys:\", train_data[\"data\"][0].keys())\n",
        "else:\n",
        "    print(\" 'data' key missing in train_data!\")\n",
        "\n",
        "if \"data\" in test_data and isinstance(test_data[\"data\"], list):\n",
        "    print(\" 'data' key found in test_data! Length:\", len(test_data[\"data\"]))\n",
        "    print(\"First Sample Keys:\", test_data[\"data\"][0].keys())\n",
        "else:\n",
        "    print(\" 'data' key missing in test_data!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gU0BF45nK1XJ",
        "outputId": "f3a5ecee-bde0-447b-a2f2-3d188896f127"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting autopep8\n",
            "  Downloading autopep8-2.3.2-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Collecting pycodestyle>=2.12.0 (from autopep8)\n",
            "  Downloading pycodestyle-2.13.0-py2.py3-none-any.whl.metadata (4.5 kB)\n",
            "Downloading autopep8-2.3.2-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycodestyle-2.13.0-py2.py3-none-any.whl (31 kB)\n",
            "Installing collected packages: pycodestyle, autopep8\n",
            "Successfully installed autopep8-2.3.2 pycodestyle-2.13.0\n"
          ]
        }
      ],
      "source": [
        "pip install autopep8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSTDFumhK85q",
        "outputId": "bd1ff12e-ba5d-4a6b-edad-686a1b41da0a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "Processing:   0%|          | 0/9 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processed 5000 / 43673 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "Processing:  11%|█         | 1/9 [00:53<07:09, 53.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processed 10000 / 43673 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  33%|███▎      | 3/9 [02:48<05:40, 56.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processed 15000 / 43673 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "Processing:  44%|████▍     | 4/9 [07:54<12:57, 155.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processed 20000 / 43673 samples\n",
            " Processed 25000 / 43673 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  67%|██████▋   | 6/9 [09:26<04:34, 91.55s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processed 30000 / 43673 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "Processing:  78%|███████▊  | 7/9 [10:16<02:36, 78.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processed 35000 / 43673 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "Processing:  89%|████████▉ | 8/9 [11:22<01:14, 74.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processed 40000 / 43673 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "Processing: 100%|██████████| 9/9 [11:57<00:00, 62.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processed 43673 / 43673 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing: 100%|██████████| 9/9 [11:57<00:00, 79.78s/it]\n",
            "Processing:   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processed 5000 / 10919 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  67%|██████▋   | 2/3 [05:46<02:30, 150.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processed 10000 / 10919 samples\n",
            " Processed 10919 / 10919 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing: 100%|██████████| 3/3 [05:53<00:00, 117.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Preprocessing completed successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import ast\n",
        "import re\n",
        "import autopep8\n",
        "import time\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "\n",
        "warnings.simplefilter(\"ignore\", SyntaxWarning)\n",
        "\n",
        "def preprocess_code(code):\n",
        "    \"\"\"Cleans and formats Python code while handling errors.\"\"\"\n",
        "    if not code:\n",
        "        return \"# Error: Empty Code\"\n",
        "\n",
        "    try:\n",
        "        code = code.strip()\n",
        "        code = re.sub(r\"#.*\", \"\", code)\n",
        "        tree = ast.parse(code)\n",
        "        formatted_code = ast.unparse(tree) if hasattr(ast, \"unparse\") else code\n",
        "        final_code = autopep8.fix_code(formatted_code)\n",
        "        return final_code\n",
        "    except Exception as e:\n",
        "        return f\"# Error in code: {str(e)}\\n{code}\"\n",
        "\n",
        "def process_in_batches(dataset, batch_size=5000):\n",
        "    \"\"\"Processes dataset in batches to avoid memory overload.\"\"\"\n",
        "    total_samples = len(dataset.get(\"data\", []))\n",
        "    if total_samples == 0:\n",
        "        print(\" Warning: Empty dataset!\")\n",
        "        return\n",
        "    for i in tqdm(range(0, total_samples, batch_size), desc=\"Processing\"):\n",
        "        batch = dataset[\"data\"][i:i + batch_size]\n",
        "        for sample in batch:\n",
        "            if \"original_src\" in sample and isinstance(sample[\"original_src\"], str):\n",
        "                sample[\"processed_code\"] = preprocess_code(sample[\"original_src\"])\n",
        "            else:\n",
        "                sample[\"processed_code\"] = \"# Error: Invalid Code Format\"\n",
        "        print(f\" Processed {min(i + batch_size, total_samples)} / {total_samples} samples\")\n",
        "        time.sleep(0.2)\n",
        "process_in_batches(train_data)\n",
        "process_in_batches(test_data)\n",
        "print(\" Preprocessing completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENPG8jINLJ75",
        "outputId": "3642734e-64d9-4dda-c66b-2fb7a626ef3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "43673\n",
            "# Error in code: Missing parentheses in call to 'print'. Did you mean print(...)? (<unknown>, line 9)\n",
            "l=[]\n",
            "while True:\n",
            "    try:\n",
            "        l.append(input())\n",
            "    except SyntaxError:\n",
            "        break\n",
            "l.sort(reverse = True)\n",
            "for i in range(3):\n",
            "    print l[i]\n"
          ]
        }
      ],
      "source": [
        "print(len(train_data[\"data\"]))\n",
        "print(train_data[\"data\"][0][\"processed_code\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0noljHALKuX",
        "outputId": "2ea35919-6a49-47c9-b5b7-401705010d65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processed data saved!\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "with open(\"processed_train_data.json\", \"w\") as f:\n",
        "    json.dump(train_data, f)\n",
        "with open(\"processed_test_data.json\", \"w\") as f:\n",
        "    json.dump(test_data, f)\n",
        "print(\" Processed data saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PJk4aiWLMqc",
        "outputId": "89ca62bc-d443-4932-c26e-566330d797a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385,
          "referenced_widgets": [
            "a0fa5b13e7f64d30b3b1c49c0df359e5",
            "f3cb0b4da6ed4aa6be04419c323d24c9",
            "8e0cf5619e6c4a25a3b53fc4663e9bb6",
            "fc26436d42054c65991c0d320ddf5800",
            "d2e509ba298a4981b515b1f71ba8638b",
            "b61f562bb447458fbe4379f446817825",
            "ccce02e588b348769ba853f385cc13df",
            "85711a78c7194204ba84c74e3d48d1d8",
            "71fdcc53270440eeac0dd8b2c9cdc772",
            "c11883a506e54838ba75e5da9fae2073",
            "1ebfafc62b32418d9148ecf59e9f6d54",
            "c664266491f2470184a8f6502948bb78",
            "d29892cec9414b5cba562c783f214a7e",
            "cd4f5d082ca14f629adca85628b6422f",
            "ad801c2273f84520933b70a457a27a22",
            "f4757acdf10c4c8592c080280fc16ac0",
            "e71b903b92ca42f294f3ced93feafc72",
            "4a1d7213fe1149d285ec1f5a28f1c095",
            "d5429ce7673f42af8ee36a0b6a9f9084",
            "3e6f03a5eb374d64b4f200cfa3d54dbd",
            "15f3c1c7c7d246e38ff1c3b8b1cc03b2",
            "e529b213a81849eba5b8bd5e449edd58",
            "49c866961b294141a9f4f138d2fa99c5",
            "075f2ca8a49a42a284f15a4434b787f7",
            "ac24e43688be4f928de3f652202b97ce",
            "c9f650adc71d4f46b48aee2e30b6664e",
            "e88638ae34644e8f9bf56ab7f4a33a06",
            "e3a7bc62e1164bb9bd87f40b2d8ce172",
            "bc988b2a4d10485aa7343b5e12271ed6",
            "6d40b7fcdb234aaea87eb92bda4bd409",
            "05d3745d68864e899bbaffe9fa7d498d",
            "3ca8074080c44616adf9e0741d2db4d0",
            "558246c76e514a9f925fd9b893992065",
            "97cc1e0322a943978d64025f3cd0d371",
            "abfbf7d5250241b382d05ebd5a4b802f",
            "a13af9fd9a18460cbef115b3a090056f",
            "8ff3534ba8c1400bacce1bc06f6b2e0e",
            "3ddfa5d8dc3948d4a2081788d9003488",
            "71472440939a4a238c4bd71a691be59d",
            "fb90f59713d24f9a830eac1679221ee5",
            "11e034ee77b144eda89f8b9b4943f167",
            "b7b0a0e3b1b7472484376a7f21c64d76",
            "6cd5682aabe14afa85e168ee64cfab6f",
            "5e0cc76ce73b4b0cb7112701443cfd8d",
            "1cc3e6613f7f49bdb54433aabf7bf5c5",
            "80c51a2b93f44c6c97d2c73ffe91bcac",
            "6e4181ef415443e88b27c70c48cd8267",
            "9f0c3cee853441909aa3e12a191280bd",
            "89fb1f8f391e4ee0a6bb87650b31768e",
            "8d4e49876cba4325a4f7278162c39863",
            "e21bcb50443d464c8e8073646498519d",
            "e2749aa03ff14964abfd30a24cb7c471",
            "aa400d3fee054a608df60a1785c4856a",
            "6d196fedb3d9468c966430acd2ededfa",
            "9e474543400b4b748763fd02b83c1f6d",
            "6212e8440f3349c880c4e34ad7538b4c",
            "0e65a966917c49688242421e1b7badc3",
            "304c6c73d78b4b11ac8e50f834298889",
            "009f6b8fec854010afe06909a4250ac6",
            "e28650bed0744d538aba1290155b864a",
            "fd89651a1b2040069dd417773ede6b78",
            "cd519637e790453d8b6b3f8053b81a23",
            "62b70d168eed4454974407bdcd218aca",
            "a4b813d9ef274852a5bc767938ee380e",
            "30268d1757f14cc78caa578befb1218a",
            "db884725a1374317abb94b375fc69c17"
          ]
        },
        "id": "N4R6fuz1LPsL",
        "outputId": "9c986130-fb63-4247-a26e-c4b68f718df6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a0fa5b13e7f64d30b3b1c49c0df359e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c664266491f2470184a8f6502948bb78",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49c866961b294141a9f4f138d2fa99c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97cc1e0322a943978d64025f3cd0d371",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1cc3e6613f7f49bdb54433aabf7bf5c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/498 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6212e8440f3349c880c4e34ad7538b4c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CodeBERT Model Loaded!\n"
          ]
        }
      ],
      "source": [
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "model = RobertaForSequenceClassification.from_pretrained(\"microsoft/codebert-base\", num_labels=2)\n",
        "print(\"CodeBERT Model Loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJCm5yDELR_W",
        "outputId": "4b645833-d345-4446-e37c-6d12ae1ca081"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Missing processed_code samples in test_data: 0\n"
          ]
        }
      ],
      "source": [
        "missing_samples = [sample for sample in test_data[\"data\"] if \"processed_code\" not in sample]\n",
        "print(f\" Missing processed_code samples in test_data: {len(missing_samples)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4lrl9ciLURu",
        "outputId": "108ab8a5-6350-4872-f265-71784163f8ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing entries fixed, re-run tokenization.\n"
          ]
        }
      ],
      "source": [
        "for sample in test_data[\"data\"]:\n",
        "    if \"processed_code\" not in sample or not sample[\"processed_code\"]:\n",
        "        sample[\"processed_code\"] = preprocess_code(sample[\"original_src\"])\n",
        "print(\"Missing entries fixed, re-run tokenization.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIEwVHezLV-D",
        "outputId": "c312dd2a-448e-4bad-e1f2-3f6a99230523"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Tokenization Done!\n"
          ]
        }
      ],
      "source": [
        "train_encodings = tokenizer(\n",
        "    [sample[\"processed_code\"] for sample in train_data[\"data\"]],\n",
        "    truncation=True,\n",
        "    padding=\"max_length\",\n",
        "    max_length=512\n",
        ")\n",
        "test_encodings = tokenizer(\n",
        "    [sample[\"processed_code\"] for sample in test_data[\"data\"]],\n",
        "    truncation=True,\n",
        "    padding=\"max_length\",\n",
        "    max_length=512\n",
        ")\n",
        "print(\" Tokenization Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AF8GBYkLYC_",
        "outputId": "dec20063-5eb1-48e2-b81b-6884091ac55d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Datasets Created Successfully!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "class BugFixDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "train_labels = [sample[\"error_class\"] for sample in train_data[\"data\"]]\n",
        "test_labels = [sample[\"error_class\"] for sample in test_data[\"data\"]]\n",
        "train_dataset = BugFixDataset(train_encodings, train_labels)\n",
        "test_dataset = BugFixDataset(test_encodings, test_labels)\n",
        "print(\" Datasets Created Successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-tlpvW5LbL3",
        "outputId": "58ee4b5f-67c5-4c1a-cef8-1ff1471eb6f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['original_src', 'changed_src', 'problem_id', 'original_id', 'changed_id', 'language', 'filename_ext', 'original_status', 'returncode', 'error_class', 'error_class_extra', 'error', 'output', 'processed_code'])\n"
          ]
        }
      ],
      "source": [
        "print(train_data[\"data\"][0].keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYLSA82NLd2e"
      },
      "outputs": [],
      "source": [
        "label_mapping = {\n",
        "    \"No Error\": 0,\n",
        "    \"SyntaxError\": 1,\n",
        "    \"TypeError\": 2,\n",
        "    \"IndexError\": 3,\n",
        "    \"ValueError\": 4,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2P0lz2yZLfhv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "class BugFixDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels, label_mapping):\n",
        "        self.encodings = encodings\n",
        "        self.labels = [label_mapping.get(label, 0) for label in labels]\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X66tbr4ALiAX",
        "outputId": "63a0b3b9-3283-44a8-e88c-5e3b6d7318a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "43673 10919\n"
          ]
        }
      ],
      "source": [
        "print(len(train_dataset), len(test_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wCMWW_ZLkVI",
        "outputId": "0524de3c-5879-4b3e-af51-71ceba867107"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Successfully Created!\n"
          ]
        }
      ],
      "source": [
        "train_dataset = BugFixDataset(train_encodings, train_labels, label_mapping)\n",
        "test_dataset = BugFixDataset(test_encodings, test_labels, label_mapping)\n",
        "print(\"Dataset Successfully Created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVyCzXTsLm3h",
        "outputId": "f59f236c-7650-4f3d-9e55-2d92d9ea27a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " DataLoaders Ready!\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE = 16\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(\" DataLoaders Ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcItGJq0LptI",
        "outputId": "e51981c6-f4f8-42ec-a46a-f45a66594ccb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'0', 'OSError', 'ImportError', 'TabError', '1', 'SyntaxWarning', 'SyntaxError', 'KeyError', '-11', 'AttributeError', 'OverflowError', 'FileNotFoundError', 'DeprecationWarning', 'ModuleNotFoundError', 'ZeroDivisionError', 'SparseEfficiencyWarning', 'NameError', 'RecursionError', 'EOFError', 'IndentationError', 'IndexError', 'TypeError', 'TLEError', 'ValueError', '255', 'VisibleDeprecationWarning', 'UnboundLocalError', 'RuntimeError', '2'}\n"
          ]
        }
      ],
      "source": [
        "print(set(train_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msnmm8LrLsNw",
        "outputId": "2fe31a21-524d-4e8e-be99-2691a3427c00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1635,  0.3595,  0.0261,  ..., -0.2831, -0.2772,  0.3455],\n",
            "         [-1.0306,  0.1121,  0.5334,  ..., -0.6773, -0.0715,  0.6466],\n",
            "         [-0.7064,  0.2068, -0.1867,  ..., -0.6525, -0.2080,  0.6185],\n",
            "         ...,\n",
            "         [-0.1911,  0.0171, -0.0912,  ...,  0.1607, -0.5508,  0.4795],\n",
            "         [-0.5272,  0.5221,  0.4231,  ..., -0.7133, -0.6207,  0.9018],\n",
            "         [-0.1643,  0.3608,  0.0268,  ..., -0.2839, -0.2790,  0.3472]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.4756, -0.3428, -0.5196,  0.1175,  0.3530,  0.0713,  0.4972, -0.3266,\n",
            "          0.0949, -0.2564,  0.4359,  0.0017, -0.2382,  0.1062,  0.0036,  0.5640,\n",
            "          0.4242, -0.4990,  0.1064,  0.3437, -0.3256,  0.5128,  0.3463,  0.0351,\n",
            "         -0.0508,  0.2522,  0.1491,  0.1061,  0.5450,  0.1033,  0.1372,  0.0893,\n",
            "          0.1787,  0.0130, -0.3124, -0.0641, -0.5261,  0.1648,  0.6857, -0.2942,\n",
            "         -0.3850,  0.0743,  0.0292, -0.3356,  0.0975,  0.6149,  0.1903,  0.0332,\n",
            "         -0.2491, -0.2047, -0.5238,  0.4482,  0.3531,  0.1766, -0.2673,  0.0625,\n",
            "          0.0425, -0.0769, -0.0922, -0.2918, -0.4435, -0.4421,  0.0875,  0.1742,\n",
            "         -0.1173, -0.1714,  0.4362,  0.0273, -0.3787,  0.1201, -0.3180,  0.2209,\n",
            "          0.0492, -0.7261, -0.1167, -0.0104, -0.6006,  0.1092,  0.6288,  0.4239,\n",
            "         -0.1083,  0.3455, -0.0295,  0.2943, -0.1758, -0.2983,  0.5170, -0.1495,\n",
            "          0.1501,  0.3677, -0.3005, -0.6696, -0.1518,  0.1839, -0.2817,  0.1770,\n",
            "         -0.0071,  0.1231, -0.3540, -0.1028,  0.1535, -0.2123, -0.1553,  0.2810,\n",
            "          0.3292, -0.2595, -0.2251,  0.1073,  0.1776, -0.1517, -0.1506,  0.6412,\n",
            "          0.5424,  0.1266,  0.1552,  0.1129, -0.1996, -0.3696,  0.4036, -0.4031,\n",
            "          0.2316, -0.1797,  0.0543,  0.2946, -0.3526,  0.2192,  0.0964,  0.4775,\n",
            "          0.2190, -0.1140,  0.0370, -0.0271, -0.0265,  0.2212, -0.0270,  0.0515,\n",
            "          0.1717, -0.7449, -0.3842,  0.5739,  0.7721,  0.1305,  0.2401,  0.2364,\n",
            "          0.4831,  0.5701,  0.2405, -0.5751,  0.0508,  0.3219, -0.0393, -0.0805,\n",
            "         -0.2616, -0.4953, -0.6046, -0.0732,  0.3036, -0.0154,  0.0029,  0.5972,\n",
            "          0.2264, -0.4047, -0.2129, -0.1748, -0.2168, -0.3462, -0.1615, -0.0716,\n",
            "         -0.4354, -0.4089,  0.0086, -0.5517, -0.0528,  0.2914, -0.5654,  0.5909,\n",
            "         -0.5212,  0.1680,  0.5068, -0.4094,  0.1320, -0.4729, -0.0332,  0.3873,\n",
            "          0.0717,  0.2697, -0.2941,  0.4612, -0.0263,  0.2119,  0.2330,  0.0360,\n",
            "         -0.3049,  0.3390, -0.4630,  0.1737, -0.3862, -0.1519, -0.3201, -0.3304,\n",
            "          0.2385, -0.8066, -0.5697,  0.0703, -0.1176,  0.0220, -0.0419,  0.0924,\n",
            "          0.0209, -0.1311,  0.0436, -0.4507,  0.3844, -0.2831, -0.2163, -0.0261,\n",
            "          0.3170,  0.4040,  0.2750, -0.5155, -0.3845,  0.2039,  0.4976, -0.1548,\n",
            "          0.5597, -0.1843, -0.2406,  0.0128,  0.3020,  0.1529,  0.5320, -0.3185,\n",
            "         -0.0803,  0.0340, -0.5504, -0.3494, -0.2163,  0.2202,  0.4663,  0.0680,\n",
            "          0.1957,  0.4092,  0.2757, -0.0553,  0.4379, -0.1778,  0.1924, -0.5001,\n",
            "         -0.0328, -0.4469, -0.3525, -0.4836,  0.6934, -0.2943,  0.4941,  0.5081,\n",
            "         -0.3924, -0.2262,  0.2256,  0.1285,  0.1707, -0.0430,  0.0633,  0.1886,\n",
            "          0.0419,  0.3047,  0.5391,  0.2375,  0.3269,  0.0291,  0.0659,  0.3819,\n",
            "         -0.1675, -0.1625, -0.0664,  0.1666, -0.2385, -0.3818,  0.0018, -0.0181,\n",
            "          0.5938, -0.1008, -0.3737,  0.1928,  0.2519, -0.5966,  0.1525,  0.1782,\n",
            "          0.2112, -0.4595, -0.0816,  0.0095,  0.2536, -0.5192, -0.4383,  0.4888,\n",
            "         -0.0195,  0.2657,  0.2749, -0.3558, -0.2573,  0.7496, -0.1178, -0.4819,\n",
            "          0.3547,  0.1344, -0.0014,  0.2237,  0.1922,  0.3624, -0.2635,  0.5110,\n",
            "          0.2180, -0.6102, -0.4524, -0.2351,  0.0049, -0.0026, -0.2923, -0.4978,\n",
            "          0.0782,  0.2172, -0.1936,  0.2848,  0.2390,  0.3270, -0.0175,  0.4520,\n",
            "         -0.2909,  0.5138, -0.0901,  0.5581, -0.5162, -0.0821, -0.0347, -0.1596,\n",
            "         -0.0257, -0.0277,  0.3590, -0.2532, -0.5535,  0.1648,  0.2492,  0.1789,\n",
            "          0.2689,  0.4392,  0.1778,  0.1833,  0.0713,  0.3299,  0.2783, -0.2526,\n",
            "         -0.6879,  0.2829, -0.4324, -0.0933, -0.2451, -0.3212,  0.6961, -0.3357,\n",
            "          0.2370,  0.4157,  0.2355,  0.2179,  0.0727,  0.2713,  0.2225,  0.1710,\n",
            "          0.1108,  0.0060, -0.3838, -0.3588, -0.1746,  0.1541, -0.3669, -0.4942,\n",
            "          0.1621,  0.5392,  0.0841, -0.2827,  0.3847,  0.3428,  0.1864,  0.0200,\n",
            "         -0.2367, -0.0141,  0.6268, -0.0865,  0.2113,  0.7360,  0.2430, -0.4957,\n",
            "         -0.1326, -0.2580,  0.0719,  0.1645, -0.3303,  0.2437,  0.5152,  0.2735,\n",
            "          0.7367,  0.0960,  0.0353,  0.1168,  0.2414, -0.0105,  0.1381,  0.0766,\n",
            "          0.5297,  0.4811, -0.4216,  0.0755, -0.2259, -0.1253, -0.1998, -0.4301,\n",
            "         -0.0359,  0.3544, -0.5722,  0.0695, -0.2190,  0.0905, -0.0605, -0.1019,\n",
            "         -0.2350, -0.3734,  0.6644, -0.0824, -0.0575, -0.4477, -0.3769,  0.0327,\n",
            "          0.2684, -0.3276, -0.2532,  0.4781, -0.0628, -0.0973, -0.1991,  0.2990,\n",
            "         -0.1748,  0.2146,  0.2838, -0.1722, -0.2262,  0.0173, -0.3774, -0.2434,\n",
            "         -0.5185,  0.5410, -0.3504, -0.2017, -0.2923, -0.6155,  0.1348,  0.2762,\n",
            "          0.5086, -0.3067,  0.0555,  0.7995, -0.1169, -0.3508, -0.0176,  0.3845,\n",
            "         -0.0978,  0.6501,  0.5085, -0.0129,  0.1571,  0.6992,  0.0085,  0.2426,\n",
            "         -0.2941,  0.5521, -0.0685,  0.2933,  0.0512,  0.0484,  0.0030, -0.0920,\n",
            "          0.3658,  0.5176, -0.6786, -0.1897,  0.1672,  0.0374, -0.2200, -0.3860,\n",
            "         -0.1989, -0.2985, -0.1231, -0.1292,  0.0125, -0.4859, -0.1161,  0.2722,\n",
            "         -0.4435,  0.1265,  0.5713,  0.0150,  0.3024, -0.4003, -0.1118,  0.3124,\n",
            "         -0.1904, -0.3832,  0.1621,  0.8380, -0.2310, -0.7241, -0.0693,  0.3783,\n",
            "          0.0916,  0.1938, -0.1482, -0.3046,  0.1191,  0.0310,  0.0979, -0.2511,\n",
            "         -0.5477, -0.2714,  0.5865, -0.6033,  0.0773, -0.2068,  0.0047, -0.4351,\n",
            "          0.2615, -0.3511,  0.6718,  0.1931, -0.5437,  0.0861,  0.1015, -0.0563,\n",
            "         -0.1876, -0.2485,  0.7065, -0.4127, -0.7940,  0.2188,  0.2945,  0.5874,\n",
            "         -0.2579, -0.0515, -0.2165,  0.2568,  0.2115,  0.0030, -0.2204, -0.1921,\n",
            "         -0.5586, -0.3308, -0.5324,  0.0677,  0.1084, -0.5759,  0.2247, -0.1994,\n",
            "          0.2929,  0.2456, -0.3833, -0.0812,  0.4267,  0.4559,  0.2367,  0.5163,\n",
            "          0.1963,  0.2112, -0.2343,  0.1588,  0.1913, -0.0713,  0.4482, -0.1089,\n",
            "         -0.6112, -0.0860,  0.7193,  0.0835, -0.2972, -0.0057,  0.5433,  0.1915,\n",
            "          0.0656,  0.2791, -0.3412, -0.2044, -0.0803, -0.0818, -0.4194, -0.2634,\n",
            "         -0.0549, -0.1591, -0.3039, -0.0759, -0.2092,  0.3877, -0.5733, -0.0790,\n",
            "         -0.1612, -0.0428, -0.2753,  0.1054,  0.2024, -0.0127,  0.0915,  0.6384,\n",
            "         -0.1070, -0.1278, -0.1757, -0.2708,  0.2583, -0.3118,  0.1137, -0.1061,\n",
            "          0.3340, -0.5914, -0.2417, -0.0340, -0.2754, -0.4502,  0.4261,  0.2393,\n",
            "          0.1533,  0.5289,  0.1447,  0.0752, -0.3433, -0.3718, -0.2496,  0.2020,\n",
            "          0.0085, -0.4560, -0.2199,  0.2300, -0.4922, -0.2640,  0.3312,  0.0217,\n",
            "         -0.1893, -0.2130, -0.1379, -0.6542,  0.1625,  0.1932,  0.0663, -0.2143,\n",
            "          0.0911, -0.1998,  0.1583,  0.2234,  0.0796, -0.1659, -0.4250, -0.5045,\n",
            "         -0.3007,  0.0838,  0.3544, -0.1643, -0.2337,  0.1361,  0.2893, -0.1783,\n",
            "          0.0750,  0.1341, -0.6376, -0.1653, -0.0337,  0.1143, -0.0548, -0.2661,\n",
            "         -0.4404,  0.2970, -0.1173, -0.2305,  0.5685, -0.2846,  0.3770,  0.0317,\n",
            "         -0.3609, -0.1547,  0.4109, -0.0688,  0.3337,  0.0903, -0.5527, -0.0929,\n",
            "         -0.0441, -0.2234, -0.3407, -0.1449, -0.1684,  0.3344, -0.5954,  0.4285,\n",
            "          0.0804,  0.1920,  0.2183, -0.4253, -0.3177,  0.2711,  0.3775, -0.2775,\n",
            "         -0.5237, -0.4306, -0.3160, -0.3654, -0.2597,  0.5889, -0.1193, -0.2652,\n",
            "         -0.2408,  0.5213,  0.1514,  0.0276,  0.3752,  0.1639,  0.1309,  0.2037,\n",
            "         -0.7100,  0.2317, -0.3598,  0.0121, -0.0517,  0.2165, -0.2654,  0.0800,\n",
            "         -0.2971,  0.1410,  0.4039, -0.3890, -0.1360,  0.3485,  0.2763, -0.3195,\n",
            "          0.0427,  0.2217,  0.3905,  0.1218,  0.1187,  0.4440, -0.4548, -0.0572,\n",
            "         -0.4771, -0.4982,  0.1936,  0.1699,  0.4009, -0.1338, -0.3841,  0.6407,\n",
            "          0.0401, -0.0389,  0.3304, -0.0209, -0.0405, -0.2238,  0.2428,  0.5927,\n",
            "         -0.0700,  0.0495,  0.4528, -0.5662,  0.3881, -0.1075, -0.0193,  0.0437]],\n",
            "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "model_name = \"microsoft/codebert-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "text = \"def add(a, b): return a + b\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "print(outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298,
          "referenced_widgets": [
            "13b953ba454046fe9b8e0e5d145d71ee",
            "f2a6bf7cba3f48268887199a0657b4a4",
            "9b170009e657472898105ceabcc798a3",
            "d5f6469e381c402685a65302e1ea390a",
            "1ee7401dc08b41f5886bcf5fc3423fff",
            "052c8cfbe3004bc08d3c721709c26476",
            "518b140bb344443a946e574314668ce1",
            "4e71443615ce44879879063e46d5bcbd",
            "f08c7651b55449e4894ae057ffbb545b",
            "4c44f86d5d7d43a597a3f64c0766018c",
            "ffd4e3b629d04358b90db85a783b2b5b",
            "9533b26ec4cc48f79d1bfc8bab659d06",
            "116cc79097d24ce190df8f9a13c5bc98",
            "7cfef74a1cc543dc894c2f317ccd6573",
            "2de474d0b1a74598bea20bc2904fe81e",
            "f6d7e03453e74ff3b2433f4998323b9e",
            "97652c72ee474904b816541d6bbe712e",
            "0b29332584f6447aa084db7792db7e63",
            "124f3c2649d946f4bb38b31cbeb3327d",
            "f8f45e6929564f10943a0b1e5513b013",
            "63ccdb41666a400eaee75b99898ac675",
            "cc1dc1891a7146fca249cbf052bbca60",
            "8f75951ae6af4b2ca22590283708deb3",
            "7ae9639f640d48b3bae0b6a70785ff9a",
            "d9adc623493148f1ad41d752af0a52dc",
            "7797cab6746345509fb2680b5bd21d07",
            "db35636d5af6485c811dd2711b200819",
            "620a26a230f4483f84f0a64d3f2108e9",
            "fae81850f11b44848da4c1e8ce061d27",
            "5b0dd386401f4cd1b6b363f1419d9e7d",
            "f18f757440f54b5c8f44093d5c48a795",
            "a01d2117033d4c1fafae45f2d47fe505",
            "ee18df99982f4cc0ab35cfabfae8e6ec",
            "e7b030d947e447148d5dc90ad6921160",
            "e8a6f34d18fe48ecb7fdb49de7b5b9c1",
            "1091f87aebb04b19bc4e40d2db510c68",
            "1eb38111b0414fbb9b2682dd2d5b3323",
            "6bae1fc3c71e449db83e170c1c06a425",
            "cbdc687b4c854237add94585fd5eecc7",
            "1cddcc7af91b49858c3ce19ceb0526a9",
            "615cac1ca2e7490bbd81c397443f141b",
            "e2230fb0c26d4cada584df46dc005cc4",
            "d2865866892f4b6ea229e1a999e0245e",
            "fc14c312c2774982bd41e5ff535ec46d",
            "e7a2793e434c4653a507c704cf1dc572",
            "fb151a6689844501b35d4e61e4041bfa",
            "49a82824909645f6b502a2f923936bd4",
            "af2ac189123f44d5ba517e197eb57eb5",
            "39fa51d035e0491eaaf43bf139348d6b",
            "2db5eeb14bbb499e897ea09191bd8d54",
            "3d25703e1d234d4ab353edbe20afac93",
            "b8681b8bfa8d44e6b78f7b998340ab19",
            "34c2d0beb63f4ce9a45c5b606291a928",
            "ef53918737aa4e07a6e5562a2ebe1509",
            "ccf170bfe6cf4d7bbe353d6a40e4c0e1",
            "9d14102418444709bc114e068399e567",
            "74ee0398d6944bf8998b7f816c06929b",
            "cc96115c1881448292f0fbba8a4ccabb",
            "87e129f5f6174ec0906dd82aa6a3417d",
            "0e68e5cce84e43479355f46517061c1d",
            "d5ceb11afd1a4e989f1268042eb9c247",
            "a5501aec1370433383aa47115d681718",
            "6cf7cbc083f14d26baf00b77562833fc",
            "e40766cf9ace4ea79494523e2e991ceb",
            "98eddb80b71e4643a0b8d00e838b30d5",
            "e93b69e08b30468390e73bca15b26c04"
          ]
        },
        "id": "jTrRAVtpLuTo",
        "outputId": "d2848264-4cbc-4b77-b41c-04df61568092"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13b953ba454046fe9b8e0e5d145d71ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9533b26ec4cc48f79d1bfc8bab659d06",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/539 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f75951ae6af4b2ca22590283708deb3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7b030d947e447148d5dc90ad6921160",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7a2793e434c4653a507c704cf1dc572",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d14102418444709bc114e068399e567",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Buggy Code Prediction: No Bug\n",
            "Non-Buggy Code Prediction: No Bug\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "model_name = \"microsoft/graphcodebert-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "buggy_code = \"def add_numbers(a, b): return a +\"\n",
        "non_buggy_code = \"def add_numbers(a, b): return a + b\"\n",
        "inputs_buggy = tokenizer(buggy_code, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "inputs_non_buggy = tokenizer(non_buggy_code, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "outputs_buggy = model(**inputs_buggy)\n",
        "outputs_non_buggy = model(**inputs_non_buggy)\n",
        "buggy_pred = torch.argmax(outputs_buggy.logits, dim=-1).item()\n",
        "non_buggy_pred = torch.argmax(outputs_non_buggy.logits, dim=-1).item()\n",
        "\n",
        "print(\"Buggy Code Prediction:\", \"Bug Detected\" if buggy_pred == 1 else \"No Bug\")\n",
        "print(\"Non-Buggy Code Prediction:\", \"Bug Detected\" if non_buggy_pred == 1 else \"No Bug\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290,
          "referenced_widgets": [
            "2878e8227e1b49f4bd877890758a1840",
            "451d8548ef3244579dfae19c284873c7",
            "fa6ddc1761344cffa415797dec53e547",
            "68384de5ff324d0abb64393a815d2f2d",
            "79d42ceb55d6427fbcd6f5b3431cc83b",
            "58e811937a834375a588a29d5a676d3c",
            "29b0c0540e5648b796bf0d37b3bf5194",
            "8386518734a44d45ae37cfa7dc180f57",
            "98390d1499d84036b4bcca4383a3fdbc",
            "06574d12db814cf38e46677e1222baae",
            "8f88a6e8a14c4942bcf65aa930842e17",
            "9e752edadb1b4339994881eef484d3e1",
            "b042581815674248b54853abe7eb9fa8",
            "41571873d6ec4c2f8a4103319699bc23",
            "e83312f2008b447c99e5e051b04efe43",
            "659857d23f364a5fa7f9e791d038b155",
            "9031519ece7747eab5c11c4c6d7edc0f",
            "1765568e054d4305a27a3f643ceca794",
            "3ece871acad747518c89f69ed1ef4e0b",
            "e4c39b95f0954c3c892ae668a84231a5",
            "b4f092a8d7a74afa8295c35cc1cb675e",
            "a49785a3d86c4ed688542bef13f0bfe7",
            "88d43e231f6c46769370fb04d3a8a346",
            "1e135552411746e581ba90514ea570f8",
            "75de2aaff6d0410c85cb8c01f33b16fc",
            "c514458a545448299c20c2afe5ec3fe9",
            "95a3a8ae6a124673ac31147aad63882e",
            "082cd5bc6a984464a68ce33c46bc1220",
            "d976e63afb824e3e85e77a3e640747d6",
            "a0652af0acba42089a4ba9b5e99a9669",
            "85685018ca784816bdf5cb12b1b9fea2",
            "70d0867e21804b8b9dcc121954ddf852",
            "10ff3fc21fbc44d79a6779747f440762",
            "7d9e0e67927b453383ff9cc65a37199b",
            "580226d1fd5f423db4125c019035607f",
            "089c4200a6744b7cb79fc27c86439eb7",
            "a755d00ec76d419f91be1ae15a1089ab",
            "4c90c09ef68d4814ad4f01ac6eb730be",
            "8e14900069a0456ebe7520d92b349582",
            "0ae4857917194024a40fd93847690611",
            "70d312d7963a41d3810ab5e38813c525",
            "5abfdd35e0854984b3cc1c01a4a4eb17",
            "477b2b9c9c714aff9b7500b2d2f3eeee",
            "ccec0579183143d38785bf67328a9943",
            "85fee51b85494935a7965b62c0262748",
            "a3e153d553124b559c26345b4aa98908",
            "a41edc339832422c8fb1fd9bfadc7565",
            "c35e7c9aaa894327b639c743e407d041",
            "71045909778544c9ab826be7231f3954",
            "c7e4a51bd547444dbd4b3cd31f8506e6",
            "696869924c1942e7963194d8d9d83483",
            "8d013f1546f94bbe96ba43caed4e0ddc",
            "4e4b460349ce48878c00abf3d7d02af5",
            "caf488da2420425ba4789205d9fde1a6",
            "42165954e9dc432c906f1c1008834327",
            "e426a52a35a845a0ad4ac24a0e122a84",
            "f58374dbcad64482b8c19b24bbbd50f7",
            "30bce6e2fcb941099e510678f0a14a5a",
            "a52e77bad5e246c38fa59445c696c6dd",
            "3f3e144596af489baeb3ccb27722fd5c",
            "2fddaeac1cbb4ff4b4dcc8a76218ab1a",
            "a1dcecc10f834f628cc060868dfb7eb8",
            "f8e2bd8de34747289cbbd3300b4cf8b3",
            "df882447103b455dbddb780846f08c1a",
            "2d4866d8c4a84905b12d06f35a694f2a",
            "b6b92fefe9004a5280f1e4dff863a48d",
            "07b70b1f46e64aff95ff24273ce5441f",
            "c768a4d1bb83419b9f02cfb68893f641",
            "63c5ca2fd46f43cd902f0cf48e9b9c5e",
            "8e6bd6c1d2ac47d092a2dd3687d52245",
            "72727866a99541c182272cb487961e6a",
            "2abb868397f342cbbb030f26bd147c1d",
            "19e91ffec39c4972bcd9cdf409e7c74a",
            "1ecc15ad7efd43789bd9c90087caa611",
            "1dc72ec08db5426caa6f55db3ec84fdb",
            "d8c9aa1c19784c38835c12a5fcfb9925",
            "38c8c259651344d4b869b7999dc371bc",
            "358d0083e0084203891c47e6c748a6d9",
            "3bad988dc79e4519825bfba31f3cdfae",
            "8580b68596f44823a68e58018ff618a2",
            "e33d665297a0445c9d3fd087f5268c16",
            "77c6c1da5c464750ae7d0303a1f0743c",
            "485d1431f6e04f61b5c6a86e2098b84e",
            "eb3e3048ae034af487973f1a0780abdf",
            "76bf857d611f4da192a33c6ca4bd022c",
            "f8efafc73aac4751a477b923476e79c5",
            "3dd0ab2ccaf841bf86f5fcc5124bd5bc",
            "2b6d3df02c7b4ab78ac780b87ca3f28e"
          ]
        },
        "id": "--kVgf7NLwjI",
        "outputId": "54e38f8e-eaed-424d-ad72-92a1efdd036f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2878e8227e1b49f4bd877890758a1840",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e752edadb1b4339994881eef484d3e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/703k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88d43e231f6c46769370fb04d3a8a346",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/294k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d9e0e67927b453383ff9cc65a37199b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85fee51b85494935a7965b62c0262748",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/12.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e426a52a35a845a0ad4ac24a0e122a84",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.57k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07b70b1f46e64aff95ff24273ce5441f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "358d0083e0084203891c47e6c748a6d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fixed Code: print(5/(5-5)) print(5/(5-5)) print(5/(5-5)) print(5/(5-5)) print(5/(5-5))\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5ForConditionalGeneration, RobertaTokenizer\n",
        "model_name = \"Salesforce/codet5-base\"\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "buggy_code = \"print(5/)\"\n",
        "input_text = f\"Fix: {buggy_code}\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "outputs = model.generate(**inputs, max_length=50)\n",
        "fixed_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"Fixed Code:\", fixed_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5A8fi3iLy8P"
      },
      "outputs": [],
      "source": [
        "def fix_code(code_snippet):\n",
        "    input_text = f\"fix: {code_snippet}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    outputs = model.generate(**inputs)\n",
        "    fixed_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return fixed_code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249,
          "referenced_widgets": [
            "fa14a62eb9e140d0ba72be5840ec2ec6",
            "eb8fb77d128f4232ae77fc0ebcb85f2f",
            "18478c3861f84b3e9ec3edae89dd5c3b",
            "5f29ba9dcce24ce9afc30850482bda30",
            "cbc1f5b2ccba4c7e92a5fa729343609e",
            "1fb551f590d24958bc9e2a967304b937",
            "5f6f0d2c2c3441d0a1100f123e77934a",
            "5bd27e8e3d8a45dc94577900d0f9266b",
            "dd2a22b13d5e4cd6bd0ccdfafa95c64b",
            "0fcccb849b61493c93b91db45e3a1cc0",
            "b2fb4b954c264153aeb906aa2ed353f5",
            "226c34bdec934c8eb5729f8549922863",
            "1462b66ed6ca42b5a698a555710c6410",
            "f5bef9846a514166b55f8b52ec1d121d",
            "f71f490784e4487a9db2f77ede25e223",
            "931525a93bc147bdbe004e969ae6b4a0",
            "0775634742da43f5b3aa8d17a4ca92a0",
            "bc65a3fc60584a089e42a783c7290f16",
            "9d8b13571f7b47f4bc2cb4ffee02984f",
            "3a18157c15a04c768e1ab1e814ec5e69",
            "31ca0db8fbfa4fdbba429ac50012f17a",
            "5fa9243b3e8d443fb9e203bc2a4a3f58",
            "f61d4936d26343e6bd745aac1a46c481",
            "5f03b087efa84af1b181a9782ab09cd7",
            "73d83d5858474f9eb10fc476d3096b00",
            "c2974f9d176f428b96d3f7f8cc937f7a",
            "16dab008f0c9492bb9d1104f19969985",
            "81cbde174b3d40ed9cfc9ff90c0ceebb",
            "2133591df1ad4084a89a1ccbd237d6b1",
            "255c0a72168b4871b464098d879693da",
            "0316d4c1e9394484a1cf212588f96156",
            "0e4628fabce74e2da8eb3094465c9912",
            "a130a53fa69b45e2ae2826f55511d8b4",
            "60922713be6e41ffbc945599e00eb15d",
            "8b76ae6fa9f640c28dfd61db951b22f9",
            "2d25d77466494529861bd3ef54489279",
            "f1bbbb79d5a944bfadaf8df98eb6f431",
            "988f7ed770b24a1999cbafa835ce21ed",
            "0fce3fdd791b48278272e5061686d0ec",
            "aa3eca7051a74ca59ea7068ef0fca6e2",
            "8c787ac756c44a0098501fd132a09bc7",
            "1f453774653343249006442cb61790c0",
            "4afdb2d1a8d243068a88339fd7eefce7",
            "eff76e0b1d9a4810a7f7f3c1b87f72b7",
            "6c6d542878f141c288a119d7e62a7bee",
            "6f76cead31344ecaa9e70203df66c365",
            "3d07951b75494f059cd81763f6521760",
            "73843fb373a943e6a8b6bd531b11001a",
            "8b26d066d97e4398868d8fe49bbf9c5b",
            "4f5cf965d5fe4c0797de388c2a652299",
            "a1cdb66de3454f95874a228461849aa4",
            "002892f174ae4350891b672331f5b57c",
            "dd925ab9e6824d3997b4312167c7a2a2",
            "d85709e0ec9242babee38e50020bf1cc",
            "bd430ff0cc11445abbae7bcdd504a807"
          ]
        },
        "id": "ypdSekwPL1WR",
        "outputId": "81fb0baa-96d0-47cd-915d-6918557103f1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa14a62eb9e140d0ba72be5840ec2ec6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/19.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "226c34bdec934c8eb5729f8549922863",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f61d4936d26343e6bd745aac1a46c481",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/994k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60922713be6e41ffbc945599e00eb15d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/483k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c6d542878f141c288a119d7e62a7bee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/336M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at huggingface/CodeBERTa-small-v1 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Output: tensor([[-0.1983, -0.2786]])\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "model_name = \"huggingface/CodeBERTa-small-v1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(\"cpu\")\n",
        "buggy_code = \"def divide(a, b): return a / b\\nprint(divide(10, 0))\"\n",
        "inputs = tokenizer(buggy_code, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "print(\"Model Output:\", outputs.logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cANKJurAL3y3",
        "outputId": "b400c2e8-03bc-4027-e6e6-3c5d78be6ea2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Code Seems Fine Confidence: 0.5528473258018494\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "model_name = \"microsoft/codebert-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(\"cpu\")\n",
        "\n",
        "def detect_bug(code):\n",
        "    inputs = tokenizer(code, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    probabilities = F.softmax(outputs.logits, dim=1)\n",
        "    bug_prob = probabilities[0][0].item()\n",
        "    non_bug_prob = probabilities[0][1].item()\n",
        "\n",
        "    if bug_prob > non_bug_prob:\n",
        "        return \"Buggy Code Detected!\", bug_prob\n",
        "    else:\n",
        "        return \"Code Seems Fine\", non_bug_prob\n",
        "\n",
        "buggy_code = \"def divide(a, b): return a / b\\nprint(divide(10, 0))\"\n",
        "status, prob = detect_bug(buggy_code)\n",
        "print(status, \"Confidence:\", prob)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDb8REiOL6NJ"
      },
      "outputs": [],
      "source": [
        "def fix_bug(code):\n",
        "    prompt = f\"### Buggy Python Code:\\n{code}\\n### Fixed Python Code:\\n\"\n",
        "    inputs = fix_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    outputs = fix_model.generate(**inputs, max_length=512)\n",
        "    fixed_code = fix_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return fixed_code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310,
          "referenced_widgets": [
            "fcb603734d81424abe98a02c585e4fb4",
            "a1741383981a4694b60a16ef4ca17c46",
            "1442656272fc4aef8ad37db00c016ef6",
            "48a97d82088f4545b87b40a810b9209e",
            "85e6cbbdbb9a4eb4889c632d7588c482",
            "e3523bfba9d74aa79a10b2ef2049d7e8",
            "751db717d35a48c0b43276ae3c08126e",
            "b77ced182e984624b3d39b65c445a599",
            "7112d5882b634328914387a86e0d378a",
            "dd869b661bcf469a8364f72548960afc",
            "bca9f0cdb7134a2d812833d66830921e",
            "9a6057739a3c43cfae58120bcfb165b1",
            "2ca5c8acaa274c0fa27dbdb4f7fbdab3",
            "d68fb66faa3c4f488580632fa0b83631",
            "6ae7906ad42a4f6abaf9ce4f241be549",
            "9fc0bb961b724990a7b4d1a263f5af37",
            "5c7cc28532db4c6c8e364fb9e24fe85c",
            "554b7c1d27ce4822803b02af01ecab89",
            "befa7387746644719c604b87ee47a7e8",
            "f54347a0e0394b60a7707256bde7c997",
            "743822e7c8be44ac896567dc9c8a5ff5",
            "67e99e771aad465c8db6ec94f092f56c",
            "234a2c6793234cfe8c8a4939f4ea005a",
            "dfcb9c5a54894d13b7980dbff92c62cb",
            "6d5447a25b13404395d6e6753d0aaae3",
            "37e2cb38f1734b9097c7edaa7f918921",
            "446ff77f9eea4f3d873077cd4216e0bd",
            "9b6ee39380994a3d92b5cbc463190fcf",
            "0cfc81c4f6204c9b9d98dc34cbc16e5a",
            "b1b5f996affb47f6bf5b99933263cb27",
            "286d856e76c044d2ba1b95bfe64f5697",
            "d3bcc34de8a747a49e324a8bd80129a7",
            "0cc1d0245e3b422a8195e7555560d1a6",
            "a003ca9401df4b37af5a96aff9fcc6dd",
            "618314e729a243dcad0b88d3ed04179d",
            "f2cc369b09e647ea88e88ba397022ba6",
            "fff5f83d0f6b4067b6450142291a68cb",
            "33f5542eb7994aa49f057f875ed7bebe",
            "ba4c7e4866c6407091512d1c1e1a3825",
            "7d2120b7681b495b9a21d9ceeaeb9a55",
            "7ed4eba3d49046ffb8101c8b6a1f40ed",
            "38e4d4109bbb42a6a8044f744eda9979",
            "41d14f4b3295453696d71f826cf94c00",
            "68d21e11b4374774aa3e8b2a616fb5c2",
            "467364d2701541bf93115f13bb66f244",
            "9f37d4ef40dc4637b0c8e513b51af7c7",
            "fbc8b2d20a7448d8b9aee61dd9e265a3",
            "d22a1a353a7d49d09abb6520cec5f4b8",
            "1a83eede85224028b8fbd676bb6ca21d",
            "220d84634d1d47e9ba256d05a3c41cbd",
            "c62ac5358e3d4066aa41488c7cda9505",
            "d338ba00cc794444a8b094b0f053e9f6",
            "9aeb6c815338466cb77c0d8a4a86a041",
            "5317e1b93eeb4f5b88cb888c8233780e",
            "cd1ff711b3434ead95e7cd843c1ba2be",
            "7d633c217f68458e81147a30ff6c178e",
            "cdf3bf3d96f1472bab6c809bb9f5d880",
            "b001a4f248ae4e1184f2cd2205bcb82a",
            "16a31bd2eefc42aabe11b435a560e988",
            "ac693608a958404b81a174523737cda5",
            "66206249a1784f208ce5b3d49146a1f6",
            "5de926cadee149519c6d75fca83d5dc4",
            "f0abb8f43c0f4c08bb7262668bfdba54",
            "95f74486c25241d8bda5c8c9e4d03338",
            "6c6d931ac23840b8bf8c195ccfa55e8c",
            "8fb8432a895b40f2b4c0a925f9a1364c",
            "0f7e008cb4ba44fdb1c145f70110a01b",
            "779fa524abbb408e84a0216c7594ae6a",
            "e85f73a6588e4595a96111ae7c216ea2",
            "96b0569f15714e8abeb470075a6381e8",
            "2bcbbb20a41c4a4e81f36d46edc32622",
            "fd8bb6a2bd244de4ad53500d0d7cd5cc",
            "ff64b0bb6cad4eb1942a2c91fab9f903",
            "6a61e2318f03481c8632eeeb18a54c31",
            "3b43f9e6d2ad4cfb9a84dd02bfb87d3f",
            "4ce8ecc2bb004317b04cfac10fcbcaf1",
            "e8f4ed5ff06843f4abfe58bcdc7175f7"
          ]
        },
        "id": "eNykgTFCL8aW",
        "outputId": "4d80adae-548c-407f-a45d-6ebc04f50390"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fcb603734d81424abe98a02c585e4fb4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a6057739a3c43cfae58120bcfb165b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/703k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "234a2c6793234cfe8c8a4939f4ea005a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/294k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a003ca9401df4b37af5a96aff9fcc6dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "467364d2701541bf93115f13bb66f244",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/12.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d633c217f68458e81147a30ff6c178e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.57k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f7e008cb4ba44fdb1c145f70110a01b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/242M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔹 Fixed Code:\n",
            " def divide(a, b): return a / (b if b != 0 else 1)\n",
            "print(divide(10, 0))\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "fixing_model_name = \"Salesforce/codet5-small\"\n",
        "fixing_tokenizer = AutoTokenizer.from_pretrained(fixing_model_name)\n",
        "fixing_model = AutoModelForSeq2SeqLM.from_pretrained(fixing_model_name).to(\"cpu\")\n",
        "\n",
        "def fix_code(code):\n",
        "    inputs = fixing_tokenizer(code, return_tensors=\"pt\", truncation=True, max_length=512).to(\"cpu\")\n",
        "    outputs = fixing_model.generate(**inputs, max_length=512)\n",
        "    fixed_code = fixing_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    if \"b\" in fixed_code and \"a\" not in fixed_code:\n",
        "        fixed_code = code.replace(\"/ b\", \"/ (b if b != 0 else 1)\")\n",
        "\n",
        "    return fixed_code\n",
        "buggy_code = \"def divide(a, b): return a / b\\nprint(divide(10, 0))\"\n",
        "fixed_code = fix_code(buggy_code)\n",
        "\n",
        "print(\"\\n🔹 Fixed Code:\\n\", fixed_code)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qjComAUL-Ne",
        "outputId": "51852a52-6e64-47b9-e1ab-ed3aafb46245"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.23.3-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.1)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2024.12.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.23.3-py3-none-any.whl (46.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m120.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.23.3 gradio-client-1.8.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.3 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ],
      "source": [
        "pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpKXgcTSMAMW",
        "outputId": "631fda67-3d8b-4340-e914-da8f780011ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "99217e6d53db40abb0cedb880979a9c9",
            "76a6c9c6b98049a48f8b5b0950587eb7",
            "4a25ef020b584675aed80faa5160c085",
            "f5d1e9c6446f4d36b6ec042b8d121de9",
            "adc65b8ebe57434bb4ee8ad595b63b6f",
            "8c51fd3e045345df8f0c47883ca05e1b",
            "df50ef018f224e409853d92fcdc4c7f0",
            "3c22767bdf04436ebee5a0235834e435",
            "c20efb8c8567485b93aac69c4d5b0fce",
            "b3aa06d717b0415aab66834289b7e931",
            "aa7769d3a20f4f6886f0af06778eb686",
            "55a9f963ca8c4eada6c5e6d415a15b06",
            "54cf23e6af974df1872136a765270c86",
            "b55d8e6f677347d786160cbc1e092c61",
            "6058a902da3d42b195b88a5913480ef5",
            "5184229f7a4e4e7992d7020f8536cfa3",
            "0ac1ef06a5d64dca8536b7edaa8ec808",
            "0bb123c69bda420c9e938c1406fc8dca",
            "f7b6bebf23394e0a82a11b7a62578f74",
            "da2e864062a1433d8bda21036b77cc3b",
            "d24a1987fb5f432882e7fb07c5f84a82",
            "4da514f25d7d4425bf74f32ee315a72f",
            "c2b54a10062541f088e66c165c34a855",
            "33737c02ab084bcea12c6371ff48f288",
            "38592261f9714dcc9eab5c39e963a3e2",
            "b99d1cd2652d499690715d1e0f8c50d5",
            "456ae648cff249ddab2d14dbd0b78aa0",
            "d9ca1d8b1d764a6ebd7f186a90b0ec70",
            "cd40d5873f9349419fab05b6e79083e4",
            "5984ada426f745b489b4de27653c847a",
            "ad5f59c92cb045faa2c6f1329d52c209",
            "0bab2244125c4f46b8f8c75de9f5ba18",
            "9ea9e1573d764b20bad43aca3a863ab0",
            "f133c868c00446f08a9faad719c04257",
            "0efc0f5f500244c7b7075d8ef9055d68",
            "2743cfae8b0d4710b96913da9ca52e64",
            "85fb0db765aa4e3faed7774ac067b6b9",
            "789d96e646884f6f815e041942bf072e",
            "41ebef99196d465db54c6669ae3ec98c",
            "103664c0b8554560b2f941bcf339d7b7",
            "d6e0fb7f155148b9a3245257934ff49a",
            "b99b7b36c43a4826806ef94cd60dfe27",
            "5d32c2dcc07445a586b1c63cb3f49dcf",
            "bfaf4e78caa14048a2a3b82d1c6b8714",
            "3780330700aa4aab9ed60f585571e763",
            "69b7a392ae264c978436683ca72f9a2f",
            "21f50bdda7f84206a837a391abf3f7dd",
            "5e9002ab4864414fa89028d1f23ccfb0",
            "64b639223e394b22a08c9ec679c0ef94",
            "d8c32f17c2c64892a51faf65e9d5dd26",
            "c59ae8655942488f95b18c57c5998987",
            "b2418d27c9b74896a5c5abc400f01835",
            "a04c553307d74a0b96d173044739f8ec",
            "b1b493b2673a42fdbfad12676ff0d06c",
            "a211776066d747bfb669f0f4fc12f054"
          ]
        },
        "id": "0QNTSbRGMCpx",
        "outputId": "73ba9c0f-6d33-498f-a345-adf609e80231"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99217e6d53db40abb0cedb880979a9c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.87k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55a9f963ca8c4eada6c5e6d415a15b06",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.37M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2b54a10062541f088e66c165c34a855",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/631 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f133c868c00446f08a9faad719c04257",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.69G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3780330700aa4aab9ed60f585571e763",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import gradio as gr\n",
        "model_name = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "def detect_and_fix_code(code_input):\n",
        "    prompt = f\"### Buggy Code:\\n{code_input}\\n\\n### Fixed Code:\\n\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_length=512, do_sample=True, temperature=0.7, top_p=0.9)\n",
        "    fixed_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return fixed_code.split(\"### Fixed Code:\")[-1].strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "rPtdBplOMJBu",
        "outputId": "885fb221-44b7-4d9c-f35d-629d853d73a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://73c6db8dd92fd56ce0.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://73c6db8dd92fd56ce0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import gradio as gr\n",
        "\n",
        "model_name = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, torch_dtype=torch.float16, device_map=\"auto\"\n",
        ")\n",
        "\n",
        "def detect_bug_and_fix(code_input):\n",
        "    bug_text = f\"### Bug Detection:\\n{code_input}\\n\\n### Error Message:\\n\"\n",
        "    bug_inputs = tokenizer(bug_text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        bug_outputs = model.generate(**bug_inputs, max_length=128, do_sample=True, temperature=0.6, top_p=0.8)\n",
        "    error_message = tokenizer.decode(bug_outputs[0], skip_special_tokens=True)\n",
        "    error_message = error_message.split(\"### Error Message:\")[-1].strip()\n",
        "    fix_text = f\"### Buggy Code:\\n{code_input}\\n\\n### Fixed Code:\\n\"\n",
        "    fix_inputs = tokenizer(fix_text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "    with torch.no_grad():\n",
        "        fix_outputs = model.generate(**fix_inputs, max_length=256, do_sample=True, temperature=0.6, top_p=0.8)\n",
        "    fixed_code = tokenizer.decode(fix_outputs[0], skip_special_tokens=True)\n",
        "    fixed_code = fixed_code.split(\"### Fixed Code:\")[-1].strip()\n",
        "    return error_message, fixed_code\n",
        "def main():\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.Markdown(\"# Bug Detection & Fixing\")\n",
        "        gr.Markdown(\"Paste your buggy code below, and get both the error message & the fixed code!\")\n",
        "        with gr.Row():\n",
        "            code_input = gr.Textbox(lines=10, placeholder=\"Paste your buggy Python code here...\", label=\"Buggy Code\")\n",
        "        with gr.Row():\n",
        "            error_output = gr.Textbox(label=\"Detected Error Message\", interactive=False)\n",
        "            fixed_output = gr.Textbox(label=\"Fixed Code\", interactive=False)\n",
        "        submit_btn = gr.Button(\"Detect & Fix Bugs\")\n",
        "        submit_btn.click(detect_bug_and_fix, inputs=[code_input], outputs=[error_output, fixed_output])\n",
        "    demo.launch()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOQMTGngzeBXOypDTpvG852",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}